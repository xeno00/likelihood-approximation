{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deep Learning for Likelihood Approximation and Inference by James Morrison and Nathan Schatz We compare the ability of different architectures to approximate and generalize a mapping from a linear space to a family of probability distributions, \\;\\;\\mathcal{L}: \\boldsymbol{\\Theta}\\subseteq\\mathbb{R}^p \\to \\{f_{\\mathbf{x}}(\\boldsymbol{x};\\boldsymbol{\\theta}) : \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\} Motivation Statistical inference has laid the foundation for solving complex problems in nearly every technical field including the social sciences, economics, business, biology, engineering, and physics. 1 Some assume artificial intelligence (AI) and machine learning (ML) to be a panacea for modeling complex systems. Yet, there remains a class of problems for which a statistical, parametric approach provides important insights or guarantees to researchers, developers, operators, and managers. When modeling for inference, there is often a tradeoff between model tractability and accuracy. 2 Historically, the most accurate model was often too complex to be useful. Instead, only the class of feasible models was considered for computing estimators or testing hypotheses, with the hope that these results might be \"good enough.\" That is, accuracy was sacrificed to make the problem solvable. Machine learning presents an alternative option for closely approximating the intractable but accurate models. Although it may seem intuitive to try completely replacing a classical inference problem with a learned model, it is advantageous to use ML as an intermediate step in statistical inference. One approach is to train a neural network to learn the \"likelihood function\" from which a probabilistic measurement is distributed. 3 Some benefits to this strategy are: Interpretability and insights, which include characterization of uncertainty Ability to generalize to diverse problem settings, different measures of error, and new prior information without re-training. Furthermore, we can exploit the inductive bias of different network architectures to ensure that the approximated likelihood meets some desirable properties, such as differentiability, convexity, modality, or normality. We explore disparate methods of likelihood approximation and compare the performance of different architectures in terms of approximation and generalization as a function of depth. Background Likelihood and the Inference Problem 4 A standard approach for statistical inference is to a model measurements, \\mathbf{x} , as a function of parameters, \\boldsymbol{\\theta} , and random noise, \\mathbf{n} : \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta},\\mathbf{n}) Thus, \\mathbf{x} is a random vector with the probability density function (PDF) f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) (in the frequentist paradigm) or f_\\mathbf{x}(\\boldsymbol{x}|\\boldsymbol{\\theta}) (in the Bayesian paradigm). In the former case, \\boldsymbol{\\theta} are modeled as deterministic, unknown parameters. In the later case, \\boldsymbol{\\theta} are random variables with the prior PDF f_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}) . In either case, the likelihood function, \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta})=f_{\\mathbf{x}}(\\boldsymbol{x};\\boldsymbol{\\theta})=f_{\\mathbf{x}}(\\boldsymbol{x}|\\boldsymbol{\\theta}) is used to infer information related to parameter estimates or other decisions/hypotheses. That is, \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}^*) represents the probability of observing the values in \\mathbf{x} , given that the true parameters were \\boldsymbol{\\theta}=\\boldsymbol{\\theta}^* . The likelihood function can be used to generate a statistic for either estimating \\boldsymbol{\\theta} or for decision making. For example, estimation and hypothesis testing problems may beget the formulation: \\hat{\\theta}_{\\mathrm{MLE}}(\\mathbf{x}) = \\underset{\\theta}{\\operatorname{arg max\\,}} f(\\mathbf{x}; \\boldsymbol{\\theta}) = \\underset{\\theta}{\\operatorname{arg max\\,}} \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) \\Lambda(\\mathbf{x}) = \\frac{f(\\mathbf{x}; H_1)}{f(\\mathbf{x}; H_0)} = \\frac{\\mathcal{L}_\\mathbf{x}(H_1) }{\\mathcal{L}_\\mathbf{x}(H_0) } \\quad \\underset{H_0}{\\overset{H_1}{\\gtrless}} \\quad \\eta or in the Bayesian case: \\hat{\\theta}_{\\mathrm{MAP}}(\\mathbf{x}) = \\underset{\\theta}{\\operatorname{arg max\\,}} \\underbrace{f(\\boldsymbol{\\theta}|\\mathbf{x})}_{\\text{posterior}} = \\underset{\\theta}{\\operatorname{arg max\\,}} \\underbrace{f(\\mathbf{x}|\\boldsymbol{\\theta})}_{\\text{likelihood}} \\cdot \\frac{\\overbrace{f(\\boldsymbol{\\theta})}^{\\text{prior}}}{\\underbrace{f(\\mathbf{x})}_{\\text{evidence}}} = \\underset{\\theta}{\\operatorname{arg max\\,}} \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) \\cdot \\frac{f(\\boldsymbol{\\theta})}{f(\\mathbf{x})} \\Lambda(\\mathbf{x}) = \\frac{f(H_1|\\mathbf{x})}{f(H_0|\\mathbf{x})} = \\frac{\\mathcal{L}_\\mathbf{x}(H_1)\\cdot\\mathbb{P}\\{H_1\\} }{\\mathcal{L}_\\mathbf{x}(H_0)\\cdot\\mathbb{P}\\{H_0\\} } \\quad \\underset{H_0}{\\overset{H_1}{\\gtrless}} \\quad \\eta We proceed using the frequentist notation without loss of generality Model Intractability We have seen that the likelihood, \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) , is crucial for inference. However, inference problems arise in a diverse set of scenarios, many of which either directly impede analytic calculation of \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) or prevent computation of a useful statistic (for example if differentiation or integration with \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) is impossible). These scenarios generally fit into one or more of the following cases: 1) The noise is not independent . Without independence, cumbersome conditional probabilities must be computed. 2) The noise is not identically distributed . Different types of measurements may be combined combined - for example, requiring the product of Gaussian and non-Gaussian random variables (this is harder to handle than multiple Gaussians with different variances, though neither cases is identically distributed). 3) The noise varies with a complicated distribution . It may be hard to differentiate or integrate over the PDF because of its functional form. 4) The noise is composite, rather than additive . The measurement model is said to have composite noise if \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta},\\mathbf{n})\\neq\\boldsymbol{h}(\\boldsymbol{\\theta})+\\mathbf{n} . That is, noise is composite if it cannot be represented additively. 5) The function \\boldsymbol{h}(\\cdot) is a \"black box\" . The parametric model may not have a known analytic form. The same challenges arise as with composite noise. Furthermore, linearization is also impossible in these cases. Note that 1-3 generally arise when calculating the joint PDF from \\{f_{\\mathrm{x}_i}(x_i;\\boldsymbol{\\theta})\\}_{i=1}^n and 4-5 are particularly pathological because they directly prevent the calculation of even one f_{\\mathrm{x}_i}(x_i;\\boldsymbol{\\theta}) Classical Approaches to Approximate Inference Some classical techniques exist to deal with the intractability of the likelihood function via approximation. In the simplest scenario, Laplace's approximation fits a single Gaussian distribution to the observed data, with mean defined by the MAP estimate and variance defined by the observed Fisher information (a sampled version of Fisher information). 5 It is simple to compute at the cost of both poor approximation and generalization. Another technique is Approximate Bayesian Computation , which maps simulated samples of the distribution f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) to a space of summary statistics. 2 It's ability to approximate is limited by the selected summary statistics, and it has poor generalization because it does not actually produce density values associated with \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) . This can involve significant computation expense, and additionally requires a finite set of candidate models and parameters to be chosen in advance. Learning-Based Approaches to Approximate Inference Neural networks provide a promising method for approximating complex likelihood functions. Their ability to approximate arbitrary functions, as expressed by the Universal Approximation Theorem, implies that they can be used to approximate arbitrary valid PDFs or likelihood functions. Likelihood Approximation Networks (LANs) have recently appeared in the cognitive neuroscience literature in order to enhance Bayesian inference methods for cognitive models. 6 Since analytical likelihood functions are required for the computation of the Bayesian posterior, neuroscientists have typically approximated the likelihood with one of a few tractable cognitive models. In order to broaden the scope of considered models, some researchers have trained deep learning models consisting of multilayer perceptrons (MLPs) and convolutional neural networks (CNNs) as substitutes for the typical cognitive models. This neural network is then directly used in place of the cognitive model for Bayesian inference. Mixture Density Networks (MDNs) are neural networks with outputs that define a parametric \"mixture\" model. Rather than approximating the likelihood function using the neural network itself, MDNs output the parameters for a linear combination of kernel functions (usually Gaussians). 7 If a mixture of the kernel functions with different parameters can theoretically approximate any arbitrary function, an appropriate loss function will ideally find the optimal parameters for approximating the distribution of the input data. Normalizing Flows are a learned series of invertible transformations that transform a Gaussian distribution into an approximation of a complex distribution described by some data. 8 The parameters of the transformations themselves can be learned. Again, an appropriate loss function ideally will cause the resulting transformations to define a PDF that mimics the distribution of the training data. Note that variational autoencoders (VAEs) can approximate a PDF; however, we have the ability to sample \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta}, \\mathbf{n}) anywhere, so we already \"know\" the latent space and have exact importance samples for \"free\" Methods In order to train ML likelihood approximation models, we simulate data from an arbitrary but complicated distribution. This particular \\boldsymbol{h}(\\boldsymbol{\\theta}, \\mathbf{n}) happens to be a composite noise model but is treated as a black box for generality. Because it has composite noise, there exists a known analytic form for \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta},\\mathbf{n}) , but there does not exist an analytic form for the PDF, f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) . In particular, we take samples, \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta},\\mathbf{n}) , for \\boldsymbol{\\theta} = [\\,\\theta_1\\;\\theta_2\\,]^\\top , with n_{i,1}\\sim \\mathcal{N}(0,0.5) , n_{i,2} \\sim \\mathrm{Laplace}(0,0.5) , and n_{i,3} \\sim \\mathrm{Uniform}(-0.01,0.01) according to: x_i = \\sin(\\theta_2+n_{i1}) + \\exp \\left\\{ -\\frac{(\\theta_1+n_{i2})^2}{2} \\right\\} + n_{i3} \\quad \\forall i \\in [\\,1,M\\,] One way to think about the conditional PDF f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) is to recognize that different values of \\boldsymbol{\\theta} each generate a different PDF for \\mathbf{x} . Because there is no analytic form for f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) , it is impossible to exactly plot these distributions. However, we can perform kernel density estimation (KDE) with a large number of samples to estimate approximate density values for each x . As the number of samples approaches infinity and the KDE bandwidth approaches 0, the approximate distribution converges to the true distribution. Below, we can see the diversity of the functions that \\boldsymbol{h}(\\cdot) can generate by plotting the f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) for several values of \\boldsymbol{\\theta} . To be successful, our models need to learn not only these 3 distributions, but every distribution that \\boldsymbol{h}(\\cdot) could generate: Training data is generated as samples from different distributions with different values of \\boldsymbol{\\theta} . We generate training pairs of the following form: \\{ \\{\\boldsymbol{\\theta}_i, \\boldsymbol{x_{i,j}}=\\boldsymbol{h}(\\boldsymbol{\\theta}_i,\\mathbf{n})\\}_{i=1}^M \\}_{j=1}^{N} That is, we have diversity over values of \\boldsymbol{\\theta} with a large value of M , but we also must take N samples from each distribution to learn the distribution's shape. Note the inherent bias and tradeoff between generalization and approximation from the relative sizes of N and M in our training set Formulating the learning problem - transfer learning Our core goal is to align our network-approximated PDF as closely as possible with the true PDF of the underlying data distribution parameterized by \\boldsymbol{\\theta} . This ultimately means that our approach must be robust enough to fit not only one function, but rather the family of functions parameterized by \\boldsymbol{\\theta} . This is a significant departure from many typical applications of Deep Learning, which often seek to approximate a single data-generating function. Essentially, this means that our model must be robust enough for useful transfer learning . In other words, our model must transfer accurately within the family of PDFs parameterized by \\boldsymbol{\\theta} . Since we do not have access to the PDF of the underlying data-generating function, we cannot use a standard loss function such as KL-Divergence to compare the two PDFs. However, we do have access to a large number of training samples from the unknown distribution. If our approximated likelihood function fits the samples well, it should assign high probability to the areas with the most training samples, and low probability to the areas with fewer samples in \\boldsymbol{\\theta} -space. Note that the simplification of KL-Divergence to a sum of log probabilities is the same \"trick\" as used in the VAE evidence lower bound (ELBO) - more on this in the following sections 9 Likelihood approximation architectures to consider We consider a few approaches for approximating the complex likelihood function and evaluate their tradeoffs. By definition, a PDF must have unit area; that is, integration over its domain must result in a total probability of 1. However, the MLP-based LAN lacks the structure to guarantee this property. Method Unit Area Pros Cons ABC Approximate Flexible and non-parametric. Requires extensive simulations for each trial model; slow; requires analytical work for selecting summary statistics. LAN No Straightforward to implement. Computationally expensive for large-scale problems; data-hungry and difficult to train. MDN Yes Directly parameterizes likelihood; expresses complex distributions as a linear combination of simple distributions. Can be unstable for complex or highly multimodal distributions. Normalizing Flows Yes Flexible and expressive. Computationally intensive training; requires careful architecture design. Loss function formulation Likelihood Approximation Networks (LANs) : After fixing \\boldsymbol{\\theta} and generating samples, \\mathbf{x} , perform KDE to generate target densities for each sampled point. That is our training set has tuples of the form \\{[\\,x_i\\;\\boldsymbol{\\theta}_i\\,],y_i\\}_{i=1}^M . We train the model using the standard MSE loss function. Normalizing Flows : The normalizing flow model starts with a base distribution (a unit Gaussian). These chained invertible and differentiable transformations can be considered as one larger, more complex, invertible and differentiable transformation of the unit Gaussian. Formally: x = \\mathbf{f}(z) z \\sim \\mathcal{N}(0,1) \\mathbf{f} = \\mathbf{f}_1 \\, \\circ \\, \\mathbf{f}_2 \\, \\circ \\cdots \\circ \\, \\mathbf{f}_n f(x) = f_z(\\mathbf{f}^{-1}(x)) \\; \\text{abs} \\left\\{ \\det \\left( \\frac{\\partial \\mathbf{f}^{-1}}{\\partial x} \\right) \\right\\} We define 8 Autoregressive Spline flow layers, each with hidden dimension 64. 10 The parameters of these flow layers will be learned by the model. For training loss, we can maximize the likelihood of the observed samples under the learned PDF, which is equivalent to: \\underset{\\mathbf{f}(\\cdot)}{\\operatorname{arg min\\,}}-\\frac{1}{M} \\sum_{i=1}^M \\log \\mathbf{f}_{\\mathbf{x}}(x_i) Mixture Density Networks (MDNs) : As described above, an MDN outputs the parameters of a PDF composed of a mixture of kernel functions (in our case Gaussians). A latent parameter vector \\boldsymbol{\\theta} is chosen and sent to the input of the neural network (MLP). A batched sample \\mathbf{x} is also generated under this parameter. Each of the output neurons represents a mean, variance, or weight of one of the K kernels, resulting in a total of 3K output neurons. The MLP can be trained similarly to the Normalizing Flow model, attempting to maximize the log probability of the sample \\mathbf{x} . Unlike the Normalizing Flow model, however, the MLP output simply gives us the parameters of a Gaussian Mixture Model, which we use to construct a synthetic distribution and evaluate the log probability of the sample under the learned model. We average this objective over the M observations within \\mathbf{x} , and use backpropagation to update the MLP parameters. After training over many values of \\boldsymbol{\\theta} , our MLP should be able to take an arbitrary input \\boldsymbol{\\theta} and output the parameters of the K -component Gaussian Mixture Model that most accurately describes the true distribution f(x; \\boldsymbol{\\theta}) . Results Each model must learn function approximations for a large domain of \\boldsymbol{\\theta} values based on only a few sample points from each distribution. Below, we see the KDE of a distribution for a specified \\boldsymbol{\\theta} along with the models' learned approximations based on only a few samples from that distribution: We see that the MLP-based LAN lacks the structure to create a convincing PDF; however, both the NF and the MDN perform well for this particular \\boldsymbol{\\theta} . Generalization Comparison Our main goal is to evaluate the performance of these network architectures at different values of \\boldsymbol{\\theta} . Since each point in \\boldsymbol{\\theta} -space parameterizes a distinct data-generating function with its own unique PDF, this is the same as evaluating the trained model's out-of-domain performance. The following heat map comparison illustrates how each model's unique inductive biases contribute to its generalization performance for this data-generating process. The red rectangle represents in-domain performance, i.e., \\boldsymbol{\\theta} was sampled uniformly within the red square during the training process. The NF and MDN architectures perform significantly better than the LAN. The MLP-based LAN fails to capture the periodic effects of \\theta_1 in the data-generating process, even within the training domain. The MLP does not exploit the structure of multiple samples from the same \\boldsymbol{\\theta} as being identically distributed. That is, it makes no assumption that \\{\\boldsymbol{\\theta}_i, \\boldsymbol{x_{i,1}}=\\boldsymbol{h}(\\boldsymbol{\\theta}_i,\\mathbf{n})\\}_{i=1}^M and \\{\\boldsymbol{\\theta}_i, \\boldsymbol{x_{i,2}}=\\boldsymbol{h}(\\boldsymbol{\\theta}_i,\\mathbf{n})\\}_{i=1}^M are related. As a result, the MSE loss criterion forces the LAN towards an average bias, where it learns sensitivity to \\theta_1 but is invariant to \\theta_2 . The MDN architecture performs better along the \\theta_2 axis for in-domain data than the NF. That is, it learns better sensitivity to the periodicity of \\theta_2 . However, it fails to generalize to out-of-domain data as well as the NF model. This is due to the inductive biases of the two strategies: the MDN assumes that the distribution has the form of a Gaussian Mixture Model, while the NF assumes that the distribution can be expressed as a composition of spline flows. Although the Gaussian Mixture Model can robustly approximate any PDF, it clearly does not generalize well beyond the values of \\boldsymbol{\\theta} that it has seen. That is, the NF model identifies functional components that are consistent across the parameter space (especially with respect to \\theta_1 ), whereas the MDN just learns the optimal approximations in-sample data. Depth Scaling In addition to inductive bias for generalization we compared the effect of increasing depth on model performance. Below, the resulting loss (negative log probability of generated in-domain data) is displayed for 150 models: In general, the most substantial gains in model depth happen before a depth of 10. The MLP-based LAN saturates to the worst (highest) loss value, the normalizing flow saturates to the best (lowest) loss value, and the MDN oscillates in the middle until a depth of about 30, at which point it begins to perform worse (relative to the same learning rate). Thus, we see that depth does not provide the same dividends to the likelihood approximation problem as in other research areas. Instead, architecture selection is crucial toward effective likelihood learning. Note that the typical training/validation loss paradigm is incompatible with the likelihood approximation problem, because the NF model and MDN are unsupervised and the MLP-based LAN has training targets based on an approximation Conclusion An accurate and useful characterization of likelihood functions is crucial for solving inference problems, which enables decision-making and understanding across a vast array of research efforts and scientific pursuits. In a statistical inference setting, model accuracy is limited by analytic tractability. In this work, we compared the ability to approximate PDFs as a function of depth and generalize as a function of model inductive bias for MLP-based LANs, NF models, and MDNs. Our composite data-generating process, parameterized by the 2D vector \\boldsymbol{\\theta} , allowed us to efficiently generate training samples for these three architectures. After training several variations of these three architectures with different depths, we compared both their in-domain approximation performance and out-of-domain generalization capability using heat maps. Overall, we conclude that the normalizing flow architecture most effectively approximates PDFs and has the best generalization profile for this problem because of its ability to learn functional components of the data-generating process. The MDN could effectively approximate PDFs but suffered from a poor generalization profile. The MLP-based LAN architecture failed to approximate the PDF well, scaled poorly with depth, and was inductively biased towards only learning the effect of a single parameter. In many contexts where predictions or conclusions must be extracted from an opaque data-generating process, deep learning can be a useful supplement to classical approximate inference techniques. Although approximation sacrifices some of the guarantees provided by pure statistical inference, deep learning techniques present an opportunity for a window into understanding complex processes where classical analysis is already intractable. These techniques are powerful tools that help enable researchers across all domains to push the boundaries of tractability and explore the dynamics of more complex processes. Johnson, Richard. \u201cStatistical Inference\u201d, Encyclopedia of Mathematics. Department of Statistics, University of Wisconsin, 2016. \u21a9 Beaumont, Mark. \u201cApproximate Bayesian computation in evolution and ecology.\u201d Annual review of ecology, evolution, and systematics, 41.1, 2010. \u21a9 \u21a9 Moustakides, George, and Kalliopi Basioti. \u201cTraining neural networks for likelihood/density ratio estimation.\u201d arXiv preprint arXiv:1911.00405, 2019. \u21a9 Casella, George, and Robert Berger. Statistical Inference, 2nd ed. Duxbury, 2001. \u21a9 MacKay, David. \u201cLaplace\u2019s Method\u201d, Information Theory, Inference and Learning Algorithms, Cambridge University Press, 2003. \u21a9 Fengler, Alexander, et al. \u201cLikelihood approximation networks (LANs) for fast inference of simulation models in cognitive neuroscience.\u201d Elife, 2021. \u21a9 Bishop, Christopher. \u201cMixture density networks.\u201d 1994. \u21a9 Kobyzev, Ivan, Simon JD Prince, and Marcus A. Brubaker. \"Normalizing flows: An introduction and review of current methods.\" IEEE transactions on pattern analysis and machine intelligence, 2020. \u21a9 Torralba, Antonio, and Phillip Isola and William Freeman. \"Representation Learning,\" Foundations of Computer Vision. 2024. \u21a9 Durkan, Conor, et al. \"Neural spline flows.\" Advances in neural information processing systems, 2019. \u21a9","title":"Deep Learning for Likelihood Approximation and Inference"},{"location":"#deep-learning-for-likelihood-approximation-and-inference","text":"","title":"Deep Learning for Likelihood Approximation and Inference"},{"location":"#by-james-morrison-and-nathan-schatz","text":"We compare the ability of different architectures to approximate and generalize a mapping from a linear space to a family of probability distributions, \\;\\;\\mathcal{L}: \\boldsymbol{\\Theta}\\subseteq\\mathbb{R}^p \\to \\{f_{\\mathbf{x}}(\\boldsymbol{x};\\boldsymbol{\\theta}) : \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\}","title":"by James Morrison and Nathan Schatz"},{"location":"#motivation","text":"Statistical inference has laid the foundation for solving complex problems in nearly every technical field including the social sciences, economics, business, biology, engineering, and physics. 1 Some assume artificial intelligence (AI) and machine learning (ML) to be a panacea for modeling complex systems. Yet, there remains a class of problems for which a statistical, parametric approach provides important insights or guarantees to researchers, developers, operators, and managers. When modeling for inference, there is often a tradeoff between model tractability and accuracy. 2 Historically, the most accurate model was often too complex to be useful. Instead, only the class of feasible models was considered for computing estimators or testing hypotheses, with the hope that these results might be \"good enough.\" That is, accuracy was sacrificed to make the problem solvable. Machine learning presents an alternative option for closely approximating the intractable but accurate models. Although it may seem intuitive to try completely replacing a classical inference problem with a learned model, it is advantageous to use ML as an intermediate step in statistical inference. One approach is to train a neural network to learn the \"likelihood function\" from which a probabilistic measurement is distributed. 3 Some benefits to this strategy are: Interpretability and insights, which include characterization of uncertainty Ability to generalize to diverse problem settings, different measures of error, and new prior information without re-training. Furthermore, we can exploit the inductive bias of different network architectures to ensure that the approximated likelihood meets some desirable properties, such as differentiability, convexity, modality, or normality. We explore disparate methods of likelihood approximation and compare the performance of different architectures in terms of approximation and generalization as a function of depth.","title":"Motivation"},{"location":"#background","text":"","title":"Background"},{"location":"#likelihood-and-the-inference-problem","text":"A standard approach for statistical inference is to a model measurements, \\mathbf{x} , as a function of parameters, \\boldsymbol{\\theta} , and random noise, \\mathbf{n} : \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta},\\mathbf{n}) Thus, \\mathbf{x} is a random vector with the probability density function (PDF) f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) (in the frequentist paradigm) or f_\\mathbf{x}(\\boldsymbol{x}|\\boldsymbol{\\theta}) (in the Bayesian paradigm). In the former case, \\boldsymbol{\\theta} are modeled as deterministic, unknown parameters. In the later case, \\boldsymbol{\\theta} are random variables with the prior PDF f_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}) . In either case, the likelihood function, \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta})=f_{\\mathbf{x}}(\\boldsymbol{x};\\boldsymbol{\\theta})=f_{\\mathbf{x}}(\\boldsymbol{x}|\\boldsymbol{\\theta}) is used to infer information related to parameter estimates or other decisions/hypotheses. That is, \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}^*) represents the probability of observing the values in \\mathbf{x} , given that the true parameters were \\boldsymbol{\\theta}=\\boldsymbol{\\theta}^* . The likelihood function can be used to generate a statistic for either estimating \\boldsymbol{\\theta} or for decision making. For example, estimation and hypothesis testing problems may beget the formulation: \\hat{\\theta}_{\\mathrm{MLE}}(\\mathbf{x}) = \\underset{\\theta}{\\operatorname{arg max\\,}} f(\\mathbf{x}; \\boldsymbol{\\theta}) = \\underset{\\theta}{\\operatorname{arg max\\,}} \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) \\Lambda(\\mathbf{x}) = \\frac{f(\\mathbf{x}; H_1)}{f(\\mathbf{x}; H_0)} = \\frac{\\mathcal{L}_\\mathbf{x}(H_1) }{\\mathcal{L}_\\mathbf{x}(H_0) } \\quad \\underset{H_0}{\\overset{H_1}{\\gtrless}} \\quad \\eta or in the Bayesian case: \\hat{\\theta}_{\\mathrm{MAP}}(\\mathbf{x}) = \\underset{\\theta}{\\operatorname{arg max\\,}} \\underbrace{f(\\boldsymbol{\\theta}|\\mathbf{x})}_{\\text{posterior}} = \\underset{\\theta}{\\operatorname{arg max\\,}} \\underbrace{f(\\mathbf{x}|\\boldsymbol{\\theta})}_{\\text{likelihood}} \\cdot \\frac{\\overbrace{f(\\boldsymbol{\\theta})}^{\\text{prior}}}{\\underbrace{f(\\mathbf{x})}_{\\text{evidence}}} = \\underset{\\theta}{\\operatorname{arg max\\,}} \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) \\cdot \\frac{f(\\boldsymbol{\\theta})}{f(\\mathbf{x})} \\Lambda(\\mathbf{x}) = \\frac{f(H_1|\\mathbf{x})}{f(H_0|\\mathbf{x})} = \\frac{\\mathcal{L}_\\mathbf{x}(H_1)\\cdot\\mathbb{P}\\{H_1\\} }{\\mathcal{L}_\\mathbf{x}(H_0)\\cdot\\mathbb{P}\\{H_0\\} } \\quad \\underset{H_0}{\\overset{H_1}{\\gtrless}} \\quad \\eta We proceed using the frequentist notation without loss of generality","title":"Likelihood and the Inference Problem"},{"location":"#model-intractability","text":"We have seen that the likelihood, \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) , is crucial for inference. However, inference problems arise in a diverse set of scenarios, many of which either directly impede analytic calculation of \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) or prevent computation of a useful statistic (for example if differentiation or integration with \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) is impossible). These scenarios generally fit into one or more of the following cases: 1) The noise is not independent . Without independence, cumbersome conditional probabilities must be computed. 2) The noise is not identically distributed . Different types of measurements may be combined combined - for example, requiring the product of Gaussian and non-Gaussian random variables (this is harder to handle than multiple Gaussians with different variances, though neither cases is identically distributed). 3) The noise varies with a complicated distribution . It may be hard to differentiate or integrate over the PDF because of its functional form. 4) The noise is composite, rather than additive . The measurement model is said to have composite noise if \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta},\\mathbf{n})\\neq\\boldsymbol{h}(\\boldsymbol{\\theta})+\\mathbf{n} . That is, noise is composite if it cannot be represented additively. 5) The function \\boldsymbol{h}(\\cdot) is a \"black box\" . The parametric model may not have a known analytic form. The same challenges arise as with composite noise. Furthermore, linearization is also impossible in these cases. Note that 1-3 generally arise when calculating the joint PDF from \\{f_{\\mathrm{x}_i}(x_i;\\boldsymbol{\\theta})\\}_{i=1}^n and 4-5 are particularly pathological because they directly prevent the calculation of even one f_{\\mathrm{x}_i}(x_i;\\boldsymbol{\\theta})","title":"Model Intractability"},{"location":"#classical-approaches-to-approximate-inference","text":"Some classical techniques exist to deal with the intractability of the likelihood function via approximation. In the simplest scenario, Laplace's approximation fits a single Gaussian distribution to the observed data, with mean defined by the MAP estimate and variance defined by the observed Fisher information (a sampled version of Fisher information). 5 It is simple to compute at the cost of both poor approximation and generalization. Another technique is Approximate Bayesian Computation , which maps simulated samples of the distribution f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) to a space of summary statistics. 2 It's ability to approximate is limited by the selected summary statistics, and it has poor generalization because it does not actually produce density values associated with \\mathcal{L}_\\mathbf{x}(\\boldsymbol{\\theta}) . This can involve significant computation expense, and additionally requires a finite set of candidate models and parameters to be chosen in advance.","title":"Classical Approaches to Approximate Inference"},{"location":"#learning-based-approaches-to-approximate-inference","text":"Neural networks provide a promising method for approximating complex likelihood functions. Their ability to approximate arbitrary functions, as expressed by the Universal Approximation Theorem, implies that they can be used to approximate arbitrary valid PDFs or likelihood functions. Likelihood Approximation Networks (LANs) have recently appeared in the cognitive neuroscience literature in order to enhance Bayesian inference methods for cognitive models. 6 Since analytical likelihood functions are required for the computation of the Bayesian posterior, neuroscientists have typically approximated the likelihood with one of a few tractable cognitive models. In order to broaden the scope of considered models, some researchers have trained deep learning models consisting of multilayer perceptrons (MLPs) and convolutional neural networks (CNNs) as substitutes for the typical cognitive models. This neural network is then directly used in place of the cognitive model for Bayesian inference. Mixture Density Networks (MDNs) are neural networks with outputs that define a parametric \"mixture\" model. Rather than approximating the likelihood function using the neural network itself, MDNs output the parameters for a linear combination of kernel functions (usually Gaussians). 7 If a mixture of the kernel functions with different parameters can theoretically approximate any arbitrary function, an appropriate loss function will ideally find the optimal parameters for approximating the distribution of the input data. Normalizing Flows are a learned series of invertible transformations that transform a Gaussian distribution into an approximation of a complex distribution described by some data. 8 The parameters of the transformations themselves can be learned. Again, an appropriate loss function ideally will cause the resulting transformations to define a PDF that mimics the distribution of the training data. Note that variational autoencoders (VAEs) can approximate a PDF; however, we have the ability to sample \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta}, \\mathbf{n}) anywhere, so we already \"know\" the latent space and have exact importance samples for \"free\"","title":"Learning-Based Approaches to Approximate Inference"},{"location":"#methods","text":"In order to train ML likelihood approximation models, we simulate data from an arbitrary but complicated distribution. This particular \\boldsymbol{h}(\\boldsymbol{\\theta}, \\mathbf{n}) happens to be a composite noise model but is treated as a black box for generality. Because it has composite noise, there exists a known analytic form for \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta},\\mathbf{n}) , but there does not exist an analytic form for the PDF, f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) . In particular, we take samples, \\mathbf{x}=\\boldsymbol{h}(\\boldsymbol{\\theta},\\mathbf{n}) , for \\boldsymbol{\\theta} = [\\,\\theta_1\\;\\theta_2\\,]^\\top , with n_{i,1}\\sim \\mathcal{N}(0,0.5) , n_{i,2} \\sim \\mathrm{Laplace}(0,0.5) , and n_{i,3} \\sim \\mathrm{Uniform}(-0.01,0.01) according to: x_i = \\sin(\\theta_2+n_{i1}) + \\exp \\left\\{ -\\frac{(\\theta_1+n_{i2})^2}{2} \\right\\} + n_{i3} \\quad \\forall i \\in [\\,1,M\\,] One way to think about the conditional PDF f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) is to recognize that different values of \\boldsymbol{\\theta} each generate a different PDF for \\mathbf{x} . Because there is no analytic form for f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) , it is impossible to exactly plot these distributions. However, we can perform kernel density estimation (KDE) with a large number of samples to estimate approximate density values for each x . As the number of samples approaches infinity and the KDE bandwidth approaches 0, the approximate distribution converges to the true distribution. Below, we can see the diversity of the functions that \\boldsymbol{h}(\\cdot) can generate by plotting the f_\\mathbf{x}(\\boldsymbol{x};\\boldsymbol{\\theta}) for several values of \\boldsymbol{\\theta} . To be successful, our models need to learn not only these 3 distributions, but every distribution that \\boldsymbol{h}(\\cdot) could generate: Training data is generated as samples from different distributions with different values of \\boldsymbol{\\theta} . We generate training pairs of the following form: \\{ \\{\\boldsymbol{\\theta}_i, \\boldsymbol{x_{i,j}}=\\boldsymbol{h}(\\boldsymbol{\\theta}_i,\\mathbf{n})\\}_{i=1}^M \\}_{j=1}^{N} That is, we have diversity over values of \\boldsymbol{\\theta} with a large value of M , but we also must take N samples from each distribution to learn the distribution's shape. Note the inherent bias and tradeoff between generalization and approximation from the relative sizes of N and M in our training set","title":"Methods"},{"location":"#formulating-the-learning-problem-transfer-learning","text":"Our core goal is to align our network-approximated PDF as closely as possible with the true PDF of the underlying data distribution parameterized by \\boldsymbol{\\theta} . This ultimately means that our approach must be robust enough to fit not only one function, but rather the family of functions parameterized by \\boldsymbol{\\theta} . This is a significant departure from many typical applications of Deep Learning, which often seek to approximate a single data-generating function. Essentially, this means that our model must be robust enough for useful transfer learning . In other words, our model must transfer accurately within the family of PDFs parameterized by \\boldsymbol{\\theta} . Since we do not have access to the PDF of the underlying data-generating function, we cannot use a standard loss function such as KL-Divergence to compare the two PDFs. However, we do have access to a large number of training samples from the unknown distribution. If our approximated likelihood function fits the samples well, it should assign high probability to the areas with the most training samples, and low probability to the areas with fewer samples in \\boldsymbol{\\theta} -space. Note that the simplification of KL-Divergence to a sum of log probabilities is the same \"trick\" as used in the VAE evidence lower bound (ELBO) - more on this in the following sections 9","title":"Formulating the learning problem - transfer learning"},{"location":"#likelihood-approximation-architectures-to-consider","text":"We consider a few approaches for approximating the complex likelihood function and evaluate their tradeoffs. By definition, a PDF must have unit area; that is, integration over its domain must result in a total probability of 1. However, the MLP-based LAN lacks the structure to guarantee this property. Method Unit Area Pros Cons ABC Approximate Flexible and non-parametric. Requires extensive simulations for each trial model; slow; requires analytical work for selecting summary statistics. LAN No Straightforward to implement. Computationally expensive for large-scale problems; data-hungry and difficult to train. MDN Yes Directly parameterizes likelihood; expresses complex distributions as a linear combination of simple distributions. Can be unstable for complex or highly multimodal distributions. Normalizing Flows Yes Flexible and expressive. Computationally intensive training; requires careful architecture design.","title":"Likelihood approximation architectures to consider"},{"location":"#loss-function-formulation","text":"Likelihood Approximation Networks (LANs) : After fixing \\boldsymbol{\\theta} and generating samples, \\mathbf{x} , perform KDE to generate target densities for each sampled point. That is our training set has tuples of the form \\{[\\,x_i\\;\\boldsymbol{\\theta}_i\\,],y_i\\}_{i=1}^M . We train the model using the standard MSE loss function. Normalizing Flows : The normalizing flow model starts with a base distribution (a unit Gaussian). These chained invertible and differentiable transformations can be considered as one larger, more complex, invertible and differentiable transformation of the unit Gaussian. Formally: x = \\mathbf{f}(z) z \\sim \\mathcal{N}(0,1) \\mathbf{f} = \\mathbf{f}_1 \\, \\circ \\, \\mathbf{f}_2 \\, \\circ \\cdots \\circ \\, \\mathbf{f}_n f(x) = f_z(\\mathbf{f}^{-1}(x)) \\; \\text{abs} \\left\\{ \\det \\left( \\frac{\\partial \\mathbf{f}^{-1}}{\\partial x} \\right) \\right\\} We define 8 Autoregressive Spline flow layers, each with hidden dimension 64. 10 The parameters of these flow layers will be learned by the model. For training loss, we can maximize the likelihood of the observed samples under the learned PDF, which is equivalent to: \\underset{\\mathbf{f}(\\cdot)}{\\operatorname{arg min\\,}}-\\frac{1}{M} \\sum_{i=1}^M \\log \\mathbf{f}_{\\mathbf{x}}(x_i) Mixture Density Networks (MDNs) : As described above, an MDN outputs the parameters of a PDF composed of a mixture of kernel functions (in our case Gaussians). A latent parameter vector \\boldsymbol{\\theta} is chosen and sent to the input of the neural network (MLP). A batched sample \\mathbf{x} is also generated under this parameter. Each of the output neurons represents a mean, variance, or weight of one of the K kernels, resulting in a total of 3K output neurons. The MLP can be trained similarly to the Normalizing Flow model, attempting to maximize the log probability of the sample \\mathbf{x} . Unlike the Normalizing Flow model, however, the MLP output simply gives us the parameters of a Gaussian Mixture Model, which we use to construct a synthetic distribution and evaluate the log probability of the sample under the learned model. We average this objective over the M observations within \\mathbf{x} , and use backpropagation to update the MLP parameters. After training over many values of \\boldsymbol{\\theta} , our MLP should be able to take an arbitrary input \\boldsymbol{\\theta} and output the parameters of the K -component Gaussian Mixture Model that most accurately describes the true distribution f(x; \\boldsymbol{\\theta}) .","title":"Loss function formulation"},{"location":"#results","text":"Each model must learn function approximations for a large domain of \\boldsymbol{\\theta} values based on only a few sample points from each distribution. Below, we see the KDE of a distribution for a specified \\boldsymbol{\\theta} along with the models' learned approximations based on only a few samples from that distribution: We see that the MLP-based LAN lacks the structure to create a convincing PDF; however, both the NF and the MDN perform well for this particular \\boldsymbol{\\theta} .","title":"Results"},{"location":"#generalization-comparison","text":"Our main goal is to evaluate the performance of these network architectures at different values of \\boldsymbol{\\theta} . Since each point in \\boldsymbol{\\theta} -space parameterizes a distinct data-generating function with its own unique PDF, this is the same as evaluating the trained model's out-of-domain performance. The following heat map comparison illustrates how each model's unique inductive biases contribute to its generalization performance for this data-generating process. The red rectangle represents in-domain performance, i.e., \\boldsymbol{\\theta} was sampled uniformly within the red square during the training process. The NF and MDN architectures perform significantly better than the LAN. The MLP-based LAN fails to capture the periodic effects of \\theta_1 in the data-generating process, even within the training domain. The MLP does not exploit the structure of multiple samples from the same \\boldsymbol{\\theta} as being identically distributed. That is, it makes no assumption that \\{\\boldsymbol{\\theta}_i, \\boldsymbol{x_{i,1}}=\\boldsymbol{h}(\\boldsymbol{\\theta}_i,\\mathbf{n})\\}_{i=1}^M and \\{\\boldsymbol{\\theta}_i, \\boldsymbol{x_{i,2}}=\\boldsymbol{h}(\\boldsymbol{\\theta}_i,\\mathbf{n})\\}_{i=1}^M are related. As a result, the MSE loss criterion forces the LAN towards an average bias, where it learns sensitivity to \\theta_1 but is invariant to \\theta_2 . The MDN architecture performs better along the \\theta_2 axis for in-domain data than the NF. That is, it learns better sensitivity to the periodicity of \\theta_2 . However, it fails to generalize to out-of-domain data as well as the NF model. This is due to the inductive biases of the two strategies: the MDN assumes that the distribution has the form of a Gaussian Mixture Model, while the NF assumes that the distribution can be expressed as a composition of spline flows. Although the Gaussian Mixture Model can robustly approximate any PDF, it clearly does not generalize well beyond the values of \\boldsymbol{\\theta} that it has seen. That is, the NF model identifies functional components that are consistent across the parameter space (especially with respect to \\theta_1 ), whereas the MDN just learns the optimal approximations in-sample data.","title":"Generalization Comparison"},{"location":"#depth-scaling","text":"In addition to inductive bias for generalization we compared the effect of increasing depth on model performance. Below, the resulting loss (negative log probability of generated in-domain data) is displayed for 150 models: In general, the most substantial gains in model depth happen before a depth of 10. The MLP-based LAN saturates to the worst (highest) loss value, the normalizing flow saturates to the best (lowest) loss value, and the MDN oscillates in the middle until a depth of about 30, at which point it begins to perform worse (relative to the same learning rate). Thus, we see that depth does not provide the same dividends to the likelihood approximation problem as in other research areas. Instead, architecture selection is crucial toward effective likelihood learning. Note that the typical training/validation loss paradigm is incompatible with the likelihood approximation problem, because the NF model and MDN are unsupervised and the MLP-based LAN has training targets based on an approximation","title":"Depth Scaling"},{"location":"#conclusion","text":"An accurate and useful characterization of likelihood functions is crucial for solving inference problems, which enables decision-making and understanding across a vast array of research efforts and scientific pursuits. In a statistical inference setting, model accuracy is limited by analytic tractability. In this work, we compared the ability to approximate PDFs as a function of depth and generalize as a function of model inductive bias for MLP-based LANs, NF models, and MDNs. Our composite data-generating process, parameterized by the 2D vector \\boldsymbol{\\theta} , allowed us to efficiently generate training samples for these three architectures. After training several variations of these three architectures with different depths, we compared both their in-domain approximation performance and out-of-domain generalization capability using heat maps. Overall, we conclude that the normalizing flow architecture most effectively approximates PDFs and has the best generalization profile for this problem because of its ability to learn functional components of the data-generating process. The MDN could effectively approximate PDFs but suffered from a poor generalization profile. The MLP-based LAN architecture failed to approximate the PDF well, scaled poorly with depth, and was inductively biased towards only learning the effect of a single parameter. In many contexts where predictions or conclusions must be extracted from an opaque data-generating process, deep learning can be a useful supplement to classical approximate inference techniques. Although approximation sacrifices some of the guarantees provided by pure statistical inference, deep learning techniques present an opportunity for a window into understanding complex processes where classical analysis is already intractable. These techniques are powerful tools that help enable researchers across all domains to push the boundaries of tractability and explore the dynamics of more complex processes. Johnson, Richard. \u201cStatistical Inference\u201d, Encyclopedia of Mathematics. Department of Statistics, University of Wisconsin, 2016. \u21a9 Beaumont, Mark. \u201cApproximate Bayesian computation in evolution and ecology.\u201d Annual review of ecology, evolution, and systematics, 41.1, 2010. \u21a9 \u21a9 Moustakides, George, and Kalliopi Basioti. \u201cTraining neural networks for likelihood/density ratio estimation.\u201d arXiv preprint arXiv:1911.00405, 2019. \u21a9 Casella, George, and Robert Berger. Statistical Inference, 2nd ed. Duxbury, 2001. \u21a9 MacKay, David. \u201cLaplace\u2019s Method\u201d, Information Theory, Inference and Learning Algorithms, Cambridge University Press, 2003. \u21a9 Fengler, Alexander, et al. \u201cLikelihood approximation networks (LANs) for fast inference of simulation models in cognitive neuroscience.\u201d Elife, 2021. \u21a9 Bishop, Christopher. \u201cMixture density networks.\u201d 1994. \u21a9 Kobyzev, Ivan, Simon JD Prince, and Marcus A. Brubaker. \"Normalizing flows: An introduction and review of current methods.\" IEEE transactions on pattern analysis and machine intelligence, 2020. \u21a9 Torralba, Antonio, and Phillip Isola and William Freeman. \"Representation Learning,\" Foundations of Computer Vision. 2024. \u21a9 Durkan, Conor, et al. \"Neural spline flows.\" Advances in neural information processing systems, 2019. \u21a9","title":"Conclusion"}]}